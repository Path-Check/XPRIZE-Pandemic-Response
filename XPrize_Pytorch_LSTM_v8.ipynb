{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XPrize Pytorch LSTM v8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv4ob3aXz0Bl"
      },
      "source": [
        "# Pathcheck Team LSTM\n",
        "Link to [Master Spreadsheet](https://docs.google.com/document/d/1-fFRebE7bEcE4AUnVpj63r9dqJF-0PbTC95FciGVYY0/edit)\n",
        "\n",
        "## Instructions \n",
        "Before you try to start this notebook load the data from [here](https://github.com/leaf-ai/covid-xprize/tree/master/covid_xprize/examples/predictors/lstm/data)\n",
        "\n",
        "~~Make sure you click the `..` folder at the top of google collab's folder navigation tab to navigate to the root.~~ Make a new folder called `data` ~~in the root~~ in the default folder `/content/`, and upload all of the csvs there.\n",
        "\n",
        "See the `MockArgs` class to change the parameters. Don't use clr yet. i'd also recommend changing the stepsize parameter (it determines how many the gradients are retained before the optimizer update step is called). \n",
        "\n",
        "Ideally we can use tensorboardX to see the charts for error metrics but i havne't figured out how to ssh tunnel a port from a google colab yet. \n",
        "\n",
        "## Things to work on\n",
        "\n",
        "**MVP**\n",
        "\n",
        "* **Tuning the model for good performance** (these parameters seem to plateau at about 10 epochs) decreasing lr or trying with a different optimizer (sgd) would be a good place to start (if you get a good predictor please download the .pth file!)\n",
        "* add validation loop # currently trains 10 models and picks the best one.\n",
        "* ~~**finish the lstm rollout rework from keras to using the new pytorch model (see `_lstm_get_test_rollouts`)** loks like # m(features)~~\n",
        "* **When this is done ^ upload a copy of the notebook to our sandbox for the robo judge**\n",
        "* ~~figure out tensorboard for metrics~~ (others will need to test theri configs) (see the note in [welcome.ipynb](https://prcx-pathcheck4307.xprizenotebooks.org/lab/tree/tutorials/welcome.ipynb))\n",
        "\n",
        "**Ayush's pointers (Novel Qualitative contributions)**\n",
        "* ~~Use a GRU (in progress)~~\n",
        "* Use seq2seq model pytorch model [(drop in)](https://github.com/gautham20/pytorch-ts), \n",
        "* ~~use smooth l1 loss~~\n",
        "* [weight losses](https://github.com/ActiveConclusion/COVID19_mobility/blob/master/google_reports) by the mobility data in each county/region\n",
        "\n",
        "**Ablation Studies**\n",
        "* Vary the number of days parameter `NB_LOOKBACK_DAYS` (if allowed)\n",
        "* Study Different spatial region resolutions (if applicable)\n",
        "* Add or preprocess context (numerical) data (such as [weather csv](https://repo.ijs.si/vitojanko/covid-from-scratch/-/blob/master/Data/features.csv), [more xprize columns](https://docs.google.com/spreadsheets/d/1waAGAoF0NE9AUHP6094kUjG8lsCdyrM53AlzaeegQic/edit#gid=1237230743))\n",
        "\n",
        "\n",
        "**Utility/minor issues**\n",
        "* ~~double check that saved models load~~\n",
        "* get pytext-nlp to work (or use the original nlp notebook to improve the model). Pytext currently crashes colab when loaded\n",
        "* Cleaning up and ~~organizing code~~\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTS4afiXAKed",
        "outputId": "097ca64d-ff49-46f1-ab00-f924c2fd6e46"
      },
      "source": [
        "# ! pip install pytext-nlp\n",
        "# ! pip install tensorboardX\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd8r7YbxyeEN"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "import re\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
        "import warnings\n",
        "from pdb import set_trace\n",
        "from torch import nn, optim, as_tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import *\n",
        "import sklearn\n",
        "# from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import urllib.request\n",
        "# import pytext\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import shutil\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-qpeKemA_QD"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtL1PDs4y_Tf"
      },
      "source": [
        "NPI_COLUMNS = ['C1_School closing',\n",
        "               'C2_Workplace closing',\n",
        "               'C3_Cancel public events',\n",
        "               'C4_Restrictions on gatherings',\n",
        "               'C5_Close public transport',\n",
        "               'C6_Stay at home requirements',\n",
        "               'C7_Restrictions on internal movement',\n",
        "               'C8_International travel controls',\n",
        "               'H1_Public information campaigns',\n",
        "               'H2_Testing policy',\n",
        "               'H3_Contact tracing',\n",
        "               'H6_Facial Coverings']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG-DxiOhGpID"
      },
      "source": [
        "### Metric/Misc Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j52bGzbaVnu"
      },
      "source": [
        "from sklearn import metrics\n",
        "import sys\n",
        "from functools import partial\n",
        "\n",
        "def get_evaluation(y_true, y_prob, list_metrics):\n",
        "    y_pred = np.round(y_prob)\n",
        "    output = {}\n",
        "    rmse = partial(metrics.mean_squared_error, squared=False)\n",
        "    func_map = {\n",
        "        \"explained_variance\": metrics.explained_variance_score,\n",
        "        \"mean_absolute_error\": metrics.mean_absolute_error,\n",
        "        \"mean_squared_error\": metrics.mean_squared_error,\n",
        "        \"rmse\": rmse,\n",
        "        \"r2\": metrics.r2_score\n",
        "    }\n",
        "\n",
        "    for m, metric_func in func_map.items():\n",
        "        if m in list_metrics:\n",
        "            output[m] = metric_func(y_true, y_pred)\n",
        "\n",
        "    return output\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# cyclic learning rate scheduling\n",
        "\n",
        "def cyclical_lr(stepsize, min_lr=1.7e-3, max_lr=1e-2):\n",
        "\n",
        "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
        "    def scaler(x):\n",
        "        return 1.0\n",
        "\n",
        "    # Lambda function to calculate the LR\n",
        "    def lr_lambda(it):\n",
        "        return min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
        "\n",
        "    # Additional function to see where on the cycle we are\n",
        "    def relative(it, stepsize):\n",
        "        cycle = math.floor(1 + it / (2 * stepsize))\n",
        "        x = abs(it / stepsize - 2 * cycle + 1)\n",
        "        return max(0, (1 - x)) * scaler(cycle)\n",
        "\n",
        "    return lr_lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikgTDzRDIJ86"
      },
      "source": [
        "### Pytorch Model and Dataset Classes\n",
        "\n",
        "See [this thread](https://discuss.pytorch.org/t/initializing-rnn-gru-and-lstm-correctly/23605/5) for details on initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFkTTE07PgJA"
      },
      "source": [
        "#PYTORCH MODELS\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "        for name, param in m.named_parameters():\n",
        "            for idx in range(4):\n",
        "                mul = param.shape[0]//4\n",
        "                if 'weight_ih' in name:\n",
        "                    torch.nn.init.xavier_uniform_(param.data[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    torch.nn.init.orthogonal_(param.data[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0.01)\n",
        "    elif type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class ContextEncoder(nn.Module):\n",
        "  def __init__(self, nb_loopback_days, nb_context_dim, hidden_dim, args):\n",
        "    super(ContextEncoder, self).__init__()\n",
        "    self.args = args\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.nb_loopback_days = nb_loopback_days  # this is just the sequence length\n",
        "    self.nb_context_dim = nb_context_dim\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.activation = nn.Softplus()\n",
        "\n",
        "    self.rnn = nn.LSTM(self.nb_context_dim, hidden_size=self.hidden_dim, num_layers=args.n_hidden_layers, batch_first=False)\n",
        "    self.ln_rnn = nn.LayerNorm(self.hidden_dim)\n",
        "    self.hidden2out = nn.Linear(self.hidden_dim, 1)\n",
        "    # h0 = torch.zeros(1, self.args.batch_size, self.hidden_dim).to(device).float()\n",
        "    # # h1 = torch.zeros(1, self.args.batch_size, self.hidden_dim).to(device).double()\n",
        "    # self.hidden = h0\n",
        "  \n",
        "  def forward(self, sequence):\n",
        "    rnn_out, _ = self.rnn(sequence.view(self.nb_loopback_days, len(sequence), -1))\n",
        "    rnn_out = self.ln_rnn(rnn_out)\n",
        "    rnn_out = torch.transpose(rnn_out, 1, 0) #swap the batch and sequence dims back\n",
        "    pre_activation = self.hidden2out(rnn_out[:,-1])\n",
        "    pred = self.activation(pre_activation)\n",
        "    return pred\n",
        "\n",
        "  def _detach(self):\n",
        "      pass\n",
        "      # self.hidden = self.hidden.detach()\n",
        "\n",
        "class ActionEncoder(nn.Module):\n",
        "  def __init__(self, nb_loopback_days, nb_action_dim, hidden_dim, args):\n",
        "    super(ActionEncoder, self).__init__()\n",
        "    self.args = args\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.nb_loopback_days = nb_loopback_days\n",
        "    self.nb_action_dim = nb_action_dim\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "    # input size, hidden size, num_layers\n",
        "    self.rnn = nn.LSTM(self.nb_action_dim, hidden_size=self.hidden_dim, num_layers=args.n_hidden_layers, batch_first=False)\n",
        "    self.ln_rnn = nn.LayerNorm(self.hidden_dim)\n",
        "    self.hidden2out = nn.Linear(self.hidden_dim, 1)\n",
        "    # h0 = torch.zeros(1, self.args.batch_size, self.hidden_dim).to(device).double()\n",
        "    # # h1 = torch.zeros(1, self.args.batch_size, self.hidden_dim).to(device).double()\n",
        "    # self.hidden = h0 \n",
        "\n",
        "\n",
        "  def forward(self, sequence):\n",
        "    rnn_out, _ = self.rnn(sequence.view(self.nb_loopback_days, len(sequence), -1))\n",
        "    rnn_out = self.ln_rnn(rnn_out)\n",
        "    rnn_out = torch.transpose(rnn_out, 1, 0) #swap the batch and sequence dims back\n",
        "    pre_activation = self.hidden2out(rnn_out[:,-1])\n",
        "    pred = self.activation(pre_activation)\n",
        "    return pred\n",
        "\n",
        "  def _detach(self):\n",
        "      pass\n",
        "      # self.hidden = self.hidden.detach()\n",
        "  \n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, nb_loopback_days, nb_action_dim, nb_context_dim, hidden_dim, args):\n",
        "      super(CombinedModel, self).__init__()\n",
        "      self.args = args\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.nb_loopback_days = nb_loopback_days\n",
        "      self.nb_action_dim = nb_action_dim\n",
        "      self.nb_context_dim = nb_context_dim\n",
        "\n",
        "      self.action_encoder = ActionEncoder(nb_loopback_days, nb_action_dim, args.hidden_dim_action, args).float()\n",
        "      self.context_encoder = ContextEncoder(nb_loopback_days, nb_context_dim, hidden_dim, args).float()\n",
        "\n",
        "      self.lambda_layer = _combine_r_and_d\n",
        "      self.apply(init_weights)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "      \"\"\"\n",
        "      take in a concatenated sequcne in the form [context, action], return prediction\n",
        "      \"\"\"\n",
        "      context, action = torch.split(sequence, [self.nb_context_dim, self.nb_action_dim], dim=2)\n",
        "      c = self.context_encoder(context)\n",
        "      a = self.action_encoder(action)\n",
        "      return self.lambda_layer((c,a))\n",
        "\n",
        "    def _detach(self):\n",
        "        self.context_encoder._detach()\n",
        "        self.action_encoder._detach()\n",
        "\n",
        "    @torch.no_grad() \n",
        "    def predict(self, input):\n",
        "        \"\"\"\n",
        "        Predict the output for a single data input\n",
        "        input shapes (TEST_DAYS, LOOPBACK_DAYS, feature_dim)\n",
        "        \"\"\"\n",
        "        [context_input, action_input] = input\n",
        "        data = XPrizeDataset(context_input, action_input, self.args, zero_pad = True)\n",
        "        prediction_loader = DataLoader(data, batch_size=self.args.batch_size, shuffle=False)\n",
        "        _, (feats, _) = list(enumerate(prediction_loader))[0]\n",
        "        pred = self.forward(feats).squeeze(dim=-1)\n",
        "        pred = pred.cpu().detach().numpy()[0] #remove dummy inputs for batch prediction\n",
        "        return pred\n",
        "\n",
        "class XPrizeDataset(Dataset):\n",
        "    def __init__(self, context, action, args, label=None, zero_pad=False, transform=torch.from_numpy):\n",
        "        batch_size, to_sequence = args.batch_size, args.to_sequence\n",
        "        self.args = args\n",
        "        self.context = context.astype(float)\n",
        "        self.action = action.astype(float)\n",
        "        if to_sequence:\n",
        "            # NOTE assumes inputs are given in order\n",
        "            label_builder = []\n",
        "            for i in range(len(self.context) - args.nb_lookback_days -1):\n",
        "                label_builder.append(label[i + 1 : i + 1 + args.nb_lookback_days] if label is not None else np.zeros(args.nb_lookback_days))\n",
        "            label = np.array(label_builder)\n",
        "\n",
        "        # print(f\"pre reshaping for dataset action shape {self.action.shape}\")\n",
        "        if zero_pad: #pad with 0 entries\n",
        "            seq_len, seq_depth, feature_dim = self.context.shape\n",
        "            self.context = np.vstack((context, np.zeros((np.abs(batch_size - seq_len), seq_depth, feature_dim))))\n",
        "            # print(self.context.shape)\n",
        "            seq_len, seq_depth, feature_dim = self.action.shape\n",
        "            self.action = np.vstack((action, np.zeros((np.abs(batch_size - seq_len), seq_depth, feature_dim))))\n",
        "            # print(self.action.shape)\n",
        "            label = np.zeros(len(self.context)) if not to_sequence else np.vstack((label, np.zeros((np.abs(batch_size - len(label)), label.shape[1]))))\n",
        "        self.label = label.astype(float)\n",
        "        self.transform = transform\n",
        "        # print(f\"dataset conext {self.context.shape}\")\n",
        "        # print(f\"dataset action {self.action.shape}\")\n",
        "        # print(f\"dataset label {self.label.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        selected_contexts = self.context[idx]\n",
        "        selected_actions = self.action[idx]\n",
        "        labels = self.label[idx]\n",
        "        inputs = np.hstack((selected_contexts, selected_actions))\n",
        "        if self.transform:\n",
        "          # pre_shape = inputs.shape\n",
        "          inputs = self.transform(inputs).float()\n",
        "          labels = torch.tensor(labels).float()\n",
        "        return inputs, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERASXV1cdgw8"
      },
      "source": [
        "## Regularizers\n",
        "\n",
        "We have a special regularizer from Ayush. Both are meant to create a monotonic approach to the optimal solution. #1 is a version of knowledge distillation that prevents the gradient update from being too far form the last model's weights. #2 is from [this paper](https://arxiv.org/pdf/2006.13593.pdf) and is better for regression so that is what we will be using. They both help to disentangle different components of the loss for better updates.\n",
        "\n",
        "**Retrospective (#2) loss usage**\n",
        "\n",
        "Note: each time we update the backup models we should set K = K + K_0*0.05\n",
        "```\n",
        "# # Step 1  - model def\n",
        "model = MyModel()\n",
        "model_retr = MyModel() # an add-on backup model\n",
        "model.train()\n",
        "model_retr.eval() # dont train this\n",
        "# Step 2 - Do forward propogation\n",
        "X, Y = image_batch, label_batch\n",
        "outputs = model(X)\n",
        "stale_outputs = model_retr(X)\n",
        "labels = Y\n",
        "# Step 3 - Update retr model periodically\n",
        "if step % N == 0:\n",
        "   model_retr.load_state_dict(model.state_dict())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-zvjcGDWiVB"
      },
      "source": [
        "# Self-distillation objectives\n",
        "# Loss Version 1\n",
        "# def retr_loss_kd(outputs, stale_outputs, labels, alpha=0.7, T=1):\n",
        "#     KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
        "#                                  F.softmax(stale_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "#                   F.CRITERION(outputs, labels) * (1. - alpha) # knowledge distillation this contrasts with the second one (taek smal steps forward (divergence from the past ))\n",
        "#     return KD_loss\n",
        "# Loss Version 2 (From the paper)\n",
        "def retr_loss(outputs, stale_outputs, labels, l1_loss=None, K=2):\n",
        "    if l1_loss is None:\n",
        "        l1_loss = nn.L1Loss()\n",
        "    retr_loss = (K+1)*l1_loss(outputs, labels) - K*l1_loss(outputs, stale_outputs)\n",
        "    return retr_loss\n",
        "\n",
        "#maybe ignore 1 and just use 2 (not for regression)\n",
        "\n",
        "\n",
        "def xprize_regularizer(model, lambda_l1=0.1):\n",
        "    \"\"\" forces weights tto be positive\"\"\"\n",
        "    # with torch.enable_grad():\n",
        "    lossl1 = torch.tensor(0).float()\n",
        "    for model_param_name, model_param_value in model.named_parameters():\n",
        "        # print(model_param_value)\n",
        "        if \"action_encoder\" in model_param_name and not model_param_name.endswith(\"hidden2out.bias\"):\n",
        "            lossl1 += lambda_l1 * model_param_value.clip(max=0).sum()\n",
        "    lossl1 = lossl1.abs() \n",
        "    return lossl1  \n",
        "\n",
        "def l2_regularizer(model, reg=1e-6):\n",
        "    \"\"\" forces magnitude of weights to be small\"\"\"\n",
        "    # with torch.enable_grad():\n",
        "    l2_loss = torch.tensor(0).float()\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'bias' not in name:\n",
        "            l2_loss = l2_loss + (0.5 * reg * torch.sum(torch.pow(param, 2)))\n",
        "    return l2_loss\n",
        "\n",
        "def l1_regularizer(model, reg=1e-6):\n",
        "    \"\"\" lasso regression forces us to focus on a subset of the weights\"\"\"\n",
        "\n",
        "    # with torch.enable_grad():\n",
        "    l1_loss = torch.tensor(0).float()\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'bias' not in name:\n",
        "            l1_loss = l1_loss + (reg * torch.sum(torch.abs(param)))\n",
        "    return l1_loss\n",
        "\n",
        "def orth_regularizer(model, reg=1e-6):\n",
        "    \"\"\" forces weights to be orthogonal, TODO check to see if we should take out the fully connected weights from this penalty, and or the rnn ones\"\"\"\n",
        "    # with torch.enable_grad():\n",
        "    orth_loss = torch.tensor(0).float()\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'bias' not in name:\n",
        "            param_flat = param.view(param.shape[0], -1)\n",
        "            sym = torch.mm(param_flat, torch.t(param_flat))\n",
        "            sym -= torch.eye(param_flat.shape[0])\n",
        "            orth_loss = orth_loss + (reg * sym.abs().sum())\n",
        "    return orth_loss\n",
        "\n",
        "def call_regularizers(model, args):\n",
        "    with torch.enable_grad():\n",
        "        loss = torch.tensor(0).float()\n",
        "        if args.xprize_regularizer:\n",
        "            loss += xprize_regularizer(model, args.xprize_lambda)\n",
        "        if args.l1_regularizer:\n",
        "            assert not args.l2_regularizer, \"shouldn't use l2 and l1 reg\"  \n",
        "            loss += l1_regularizer(model, args.l1_lambda)\n",
        "        if args.l2_regularizer:\n",
        "            assert not args.l1_regularizer, \"shouldn't use l2 and l1 reg\" \n",
        "            loss += l2_regularizer(model, args.l2_lambda) \n",
        "        if args.orth_regularizer:\n",
        "            loss += orth_regularizer(model, args.orth_lambda)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VZkkwR4G0Jy"
      },
      "source": [
        "### Args Run and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucyqBmGzZJKy"
      },
      "source": [
        "class MockArgs(object):\n",
        "  def __init__(self):\n",
        "    #META\n",
        "    self.dry_run = False\n",
        "    self.flush_history = False\n",
        "    self.log_path = \"./logs/\"\n",
        "    self.model_name=\"lstm2xc4xa\"\n",
        "    self.output = \"./modelsaves/\"\n",
        "    self.log_every= 100\n",
        "    self.checkpoint = False\n",
        "\n",
        "\n",
        "    # Run loop Params\n",
        "    self.scheduler = None # one of \"clr\", \"step\"\n",
        "    self.optimizer = \"adam\" # one of \"adam\", \"sgd\"\n",
        "    self.epochs = 17\n",
        "    self.learning_rate = .008\n",
        "    self.min_lr = 1.7e-3 # CLR ONLY\n",
        "    self.max_lr = 1e-2 # CLR ONLY\n",
        "    self.early_stopping = True\n",
        "    self.patience = 3 #patience for early stopping (how long to wait for improvement)\n",
        "    self.weight_decay = 0\n",
        "    self.momentum = .8\n",
        "    self.model_selecting_metric = \"loss\" # highly recommend \"loss\", see get_evaluation() for metrics, note for some metrics higher is better, may need to change the scoring logic\n",
        "    self.criterion = \"l1\" # one of l1, smooth_l1\n",
        "    self.smooth_l1_beta = 1 # 0 = l1 loss\n",
        "\n",
        "\n",
        "    #DATA Params\n",
        "    self.batch_size = 250\n",
        "    self.shuffle = True\n",
        "    self.num_workers = 4\n",
        "    self.val_split_percentage = .2\n",
        "\n",
        "\n",
        "    ## From Xprize model params\n",
        "    self.nb_lookback_days = 21\n",
        "    self.nb_test_days = 14\n",
        "    self.window_size = 7\n",
        "    self.num_trials = 3\n",
        "    self.n_hidden_layers = 1\n",
        "    self.hidden_dim= 32\n",
        "    self.hidden_dim_action= 64\n",
        "    self.max_nb_countries = 20\n",
        "\n",
        "\n",
        "    #Regularizers\n",
        "    self.xprize_regularizer = False\n",
        "    self.xprize_lambda = 1e-6\n",
        "    self.l1_regularizer = False\n",
        "    self.l1_lambda = 1e-6\n",
        "    self.orth_regularizer = False\n",
        "    self.orth_lambda = 1e-6\n",
        "    self.l2_regularizer = False\n",
        "    self.l2_lambda = 1e-6\n",
        "    self.regularizer_ratio = .5\n",
        "    self.retr_loss= True\n",
        "    self.retr_warmup = 0\n",
        "    self.retr_burnin = [0]\n",
        "\n",
        "    if not os.path.exists(self.output):\n",
        "        os.makedirs(self.output)\n",
        "    if not os.path.exists(self.log_path):\n",
        "        os.makedirs(self.log_path)\n",
        "\n",
        "    self.stepsize = 1\n",
        "    # For seq2seq (not impl)\n",
        "    self.embedding_dim = 16 #DO NOT USE\n",
        "    self.to_sequence = False\n",
        "\n",
        "  def set_regularizers(self):\n",
        "      self.l1_lambda = self.xprize_lambda / self.regularizer_ratio\n",
        "      self.l2_lambda = self.xprize_lambda / self.regularizer_ratio\n",
        "      self.orth_lambda = self.xprize_lambda / (self.regularizer_ratio*2)\n",
        "\n",
        "def criterion_to_function(criterion):\n",
        "    if criterion == 'l1':\n",
        "        return  nn.L1Loss(reduction=\"mean\")\n",
        "    elif criterion == \"smooth_l1\":\n",
        "        return nn.SmoothL1Loss(beta=args.smooth_l1_beta)\n",
        "    elif criterion == \"mse\":\n",
        "        return nn.MSELoss()\n",
        "\n",
        "    else:\n",
        "        assert False, f\"{criterion} criterion not found\"\n",
        "\n",
        "\n",
        " \n",
        "def run(args, model, X_context, X_action, y, trial_num=0):\n",
        "\n",
        "    if args.flush_history == 1:\n",
        "        objects = os.listdir(args.log_path)\n",
        "        for f in objects:\n",
        "            if os.path.isdir(args.log_path + f):\n",
        "                shutil.rmtree(args.log_path + f)\n",
        "\n",
        "    now = datetime.now()\n",
        "    logdir = args.log_path + args.model_name + f\"_{trial_num}/\"\n",
        "    if os.path.exists(logdir):\n",
        "        shutil.rmtree(logdir)\n",
        "    os.makedirs(logdir, exist_ok=True)\n",
        "    log_file = logdir + \"log.txt\"\n",
        "    writer = SummaryWriter(logdir)\n",
        "    criterion = criterion_to_function(args.criterion)\n",
        "\n",
        "\n",
        "    if args.optimizer == \"sgd\":\n",
        "        if args.scheduler == \"clr\":\n",
        "            optimizer = torch.optim.SGD(\n",
        "                model.parameters(), lr=1, momentum=0.9, weight_decay=0.00001\n",
        "            )\n",
        "        else:\n",
        "            optimizer = torch.optim.SGD(\n",
        "                model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay\n",
        "            )\n",
        "    elif args.optimizer == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model_path = \"\"\n",
        "\n",
        "    root_set = XPrizeDataset(X_context, X_action, args, label=y, zero_pad=False)\n",
        "    val_n = int(len(root_set)*args.val_split_percentage)\n",
        "    train_n = len(root_set) - val_n\n",
        "    train_set, val_set = torch.utils.data.random_split(root_set, [train_n, val_n])\n",
        "\n",
        "    training_dataloader = DataLoader(train_set, batch_size=args.batch_size, shuffle=args.shuffle, num_workers=args.num_workers, drop_last=True)\n",
        "    val_dataloader = DataLoader(val_set, batch_size=args.batch_size, shuffle=args.shuffle, num_workers=args.num_workers, drop_last=True)\n",
        "\n",
        "    if args.scheduler == \"clr\":\n",
        "        stepsize = int(args.stepsize * len(training_dataloader))\n",
        "        clr = cyclical_lr(stepsize, args.min_lr, args.max_lr)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    model_retr = None\n",
        "    K_0 = 0\n",
        "    K = 0\n",
        "    if args.retr_loss:\n",
        "        model_retr = copy.deepcopy(model)\n",
        "        model_retr.load_state_dict(model.state_dict())\n",
        "        K_0 = 2\n",
        "        K = 2\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        if epoch % args.stepsize == 0 and epoch >= args.retr_warmup:\n",
        "            K += K_0*.15\n",
        "            # model_retr.load_state_dict(model.state_dict())\n",
        "\n",
        "        training_loss, mets, K = train(\n",
        "              model,\n",
        "              training_dataloader,\n",
        "              optimizer,\n",
        "              criterion,\n",
        "              epoch,\n",
        "              writer,\n",
        "              log_file,\n",
        "              scheduler,\n",
        "              args,\n",
        "              args.log_every,\n",
        "              model_retr,\n",
        "              K,\n",
        "          )\n",
        "        \n",
        "        val_loss, val_mets = evaluate(\n",
        "              model,\n",
        "              training_dataloader,\n",
        "              criterion,\n",
        "              epoch,\n",
        "              writer,\n",
        "              log_file,\n",
        "              args,\n",
        "              args.log_every,\n",
        "        )\n",
        "\n",
        "        update_str = \"[Epoch: {} / {}] train_loss: {:.4f} | val_loss: {:.4f}\".format(\n",
        "                epoch + 1,\n",
        "                args.epochs,\n",
        "                training_loss,\n",
        "                val_loss,\n",
        "            )\n",
        "        for m, metric in val_mets.items():\n",
        "            update_str += f\" | val {m}: {metric.avg:.4f}\"\n",
        "            if m == args.model_selecting_metric:\n",
        "                epoch_score = metric.avg\n",
        "\n",
        "        if args.model_selecting_metric == \"loss\":\n",
        "            epoch_score = val_loss\n",
        "\n",
        "        print(update_str)\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # learning rate scheduling\n",
        "\n",
        "        if args.scheduler == \"step\":\n",
        "            if args.optimizer == \"sgd\" and ((epoch + 1) % 3 == 0) and epoch > 0:\n",
        "                current_lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "                current_lr /= 2\n",
        "                print(\"Decreasing learning rate to {0}\".format(current_lr))\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group[\"lr\"] = current_lr\n",
        "\n",
        "        # model checkpoint\n",
        "\n",
        "        if epoch_score < best_score or epoch == args.epochs:\n",
        "            best_score = epoch_score \n",
        "            best_epoch = epoch\n",
        "            if args.checkpoint == 1:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    args.output\n",
        "                    + \"model_{}_epoch_{}_lr_{}_loss_{}_score_{}.pth\".format(\n",
        "                        args.model_name,\n",
        "                        epoch,\n",
        "                        optimizer.state_dict()[\"param_groups\"][0][\"lr\"],\n",
        "                        round(training_loss, 4),\n",
        "                        round(epoch_score, 4),\n",
        "                    ),\n",
        "                )\n",
        "            torch.save(model.state_dict(), f\"{args.output}{args.model_name}.pth\")\n",
        "\n",
        "        if bool(args.early_stopping):\n",
        "            if epoch - best_epoch > args.patience > 0:\n",
        "                print(\n",
        "                    \"Stop training at epoch {}. The lowest loss achieved is {} at epoch {}\".format(\n",
        "                        epoch, val_loss, best_epoch\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                break\n",
        "    model.load_state_dict(torch.load(f\"{args.output}{args.model_name}.pth\"))\n",
        "\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    training_dataloader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    epoch,\n",
        "    writer,\n",
        "    log_file,\n",
        "    scheduler,\n",
        "    args,\n",
        "    print_every=25,\n",
        "    model_retr = None,\n",
        "    K=0\n",
        "):\n",
        "    model.train()\n",
        "    do_retr = epoch >= args.retr_warmup\n",
        "    if model_retr is not None:\n",
        "        model_retr.eval()\n",
        "    losses = AverageMeter()\n",
        "    num_iter_per_epoch = len(training_dataloader)\n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "    progress_bar = tqdm(enumerate(training_dataloader), total=num_iter_per_epoch)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    LIST_METRICS = [\"rmse\", \"mean_absolute_error\", \"explained_variance\"] # add to the args object\n",
        "    meters = {m: AverageMeter() for m in LIST_METRICS}\n",
        "    if args.dry_run:\n",
        "        i, (feats, labels) = list(enumerate(training_dataloader))[0]\n",
        "        temp_out = model(feats)\n",
        "        pred_shape = temp_out.shape\n",
        "        print(f\"DRY RUNNING, prediction shape will be filled with zeros of shape {temp_out.shape} and type {temp_out.type()}\")\n",
        "        print(f\"labels are {labels.shape} and type {labels.type()}\")\n",
        "\n",
        "    for iter, batch in progress_bar:\n",
        "        features, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "            features = features.cuda()\n",
        "            labels = labels.cuda()\n",
        "        \n",
        "        features.requires_grad_()\n",
        "        optimizer.zero_grad()\n",
        "        if not args.dry_run:\n",
        "            predictions = model(features).squeeze(dim=-1)\n",
        "            if model_retr is not None and do_retr:\n",
        "                stale_predictions = model_retr(features).squeeze(dim=-1)\n",
        "            # print(predictions.shape)\n",
        "            # print(torch.squeeze(predictions, dim=-1))\n",
        "        else:\n",
        "            predictions = torch.zeros(temp_out.shape)\n",
        "\n",
        "        # y_true += labels.cpu().numpy().tolist()\n",
        "        # y_pred += predictions.cpu().detach().numpy().tolist()\n",
        "\n",
        "        loss = criterion(predictions, labels)\n",
        "        if args.retr_loss and do_retr:\n",
        "             r_loss = retr_loss(predictions, stale_predictions, labels, K=K).clip(min=0)\n",
        "             loss = (loss*1.5 + r_loss*.5) if epoch in args.retr_burnin else loss + r_loss\n",
        "        reg = call_regularizers(model, args)\n",
        "        loss += reg\n",
        "        if not args.dry_run:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step() #step\n",
        "        #detatch hidden states\n",
        "        # model._detach()\n",
        "        if args.scheduler == \"clr\":\n",
        "            scheduler.step()\n",
        "\n",
        "        training_metrics = get_evaluation(\n",
        "            labels.cpu().numpy(),\n",
        "            predictions.cpu().detach().numpy(),\n",
        "            list_metrics=LIST_METRICS,\n",
        "        )\n",
        "\n",
        "        losses.update(loss.item(), features.size(0))\n",
        "        for m, meter in meters.items():\n",
        "            meter.update(training_metrics[m], features.size(0))\n",
        "\n",
        "        writer.add_scalar(\"Train/Loss\", loss.item(), epoch * num_iter_per_epoch + iter)\n",
        "\n",
        "        for metric, value in training_metrics.items():\n",
        "            writer.add_scalar(\n",
        "                f\"Train/{metric}\",\n",
        "                value,\n",
        "                epoch * num_iter_per_epoch + iter,\n",
        "            )\n",
        "\n",
        "\n",
        "        lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "\n",
        "        if (iter % print_every == 0) and (iter > 0):\n",
        "            print(\n",
        "                \"[Training - Epoch: {}], LR: {} , Iteration: {}/{} , Loss: {}\".format(\n",
        "                    epoch + 1, lr, iter, num_iter_per_epoch, losses.avg\n",
        "                )\n",
        "            )\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    writer.add_scalar(\"Train/loss/epoch\", losses.avg, epoch)\n",
        "    for m, meter in meters.items():\n",
        "        writer.add_scalar(f\"Train/{m}/epoch\", meter.avg, epoch)\n",
        "\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(f\"Training on Epoch {epoch} \\n\")\n",
        "        f.write(f\"Average loss: {losses.avg} \\n\")\n",
        "        for m, meter in meters.items():\n",
        "            f.write(f\"Average {m}: {meter.avg} \\n\")\n",
        "        f.write(\"*\" * 25)\n",
        "        f.write(\"\\n\")\n",
        "    model.eval()\n",
        "    return losses.avg, meters, K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model,\n",
        "    val_dataloader,\n",
        "    criterion,\n",
        "    epoch,\n",
        "    writer,\n",
        "    log_file,\n",
        "    args,\n",
        "    print_every=25,\n",
        "):\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    num_iter_per_epoch = len(val_dataloader)\n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "    progress_bar = tqdm(enumerate(val_dataloader), total=num_iter_per_epoch)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    LIST_METRICS = [\"rmse\", \"mean_absolute_error\", \"explained_variance\"] # add to the args object\n",
        "    meters = {m: AverageMeter() for m in LIST_METRICS}\n",
        "    for iter, batch in progress_bar:\n",
        "        features, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "            features = features.cuda()\n",
        "            labels = labels.cuda()\n",
        "        with torch.no_grad():\n",
        "            predictions = model.forward(features).squeeze(dim=-1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            reg = call_regularizers(model, args)\n",
        "            loss += reg\n",
        "\n",
        "        #detatch hidden states\n",
        "        # model._detach()\n",
        "\n",
        "        val_metrics = get_evaluation(\n",
        "            labels.cpu().numpy(),\n",
        "            predictions.cpu().detach().numpy(),\n",
        "            list_metrics=LIST_METRICS,\n",
        "        )\n",
        "\n",
        "        losses.update(loss.item(), features.size(0))\n",
        "        for m, meter in meters.items():\n",
        "            meter.update(val_metrics[m], features.size(0))\n",
        "\n",
        "        # writer.add_scalar(\"Val/Loss\", loss.item(), epoch * num_iter_per_epoch + iter)\n",
        "\n",
        "        # for metric, value in val_metrics.items():\n",
        "        #     writer.add_scalar(\n",
        "        #         f\"Val/{metric}\",\n",
        "        #         value,\n",
        "        #         epoch * num_iter_per_epoch + iter,\n",
        "        #     )\n",
        "        # lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "\n",
        "        if (iter % print_every == 0) and (iter > 0):\n",
        "            print(\n",
        "                \"[Val - Epoch: {}], Iteration: {}/{} , Loss: {}\".format(\n",
        "                    epoch + 1, iter, num_iter_per_epoch, losses.avg\n",
        "                )\n",
        "            )\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    writer.add_scalar(\"Val/loss/epoch\", losses.avg, epoch)\n",
        "    for m, meter in meters.items():\n",
        "        writer.add_scalar(f\"Val/{m}/epoch\", meter.avg, epoch)\n",
        "\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(f\"Val on Epoch {epoch} \\n\")\n",
        "        f.write(f\"Average loss: {losses.avg} \\n\")\n",
        "        for m, meter in meters.items():\n",
        "            f.write(f\"Average {m}: {meter.avg} \\n\")\n",
        "        f.write(\"*\" * 25)\n",
        "        f.write(\"\\n\")\n",
        "    return losses.avg, meters\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMn2AbWiG8wO"
      },
      "source": [
        "### XPrize Predictor Class (csvs, rollout,etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIzybjogZ1tt"
      },
      "source": [
        "# High level flow\n",
        "## context input to the model h(x)\n",
        "## action input to the model g()\n",
        "## both input go in parallel to a LSTM model\n",
        "## LSTM model's outputs are fed to respective dense layers\n",
        "## The dense layer output is obtained\n",
        "## The final output is (1-g)h()\n",
        "\n",
        "\n",
        "# Copyright 2020 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 License.\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Suppress noisy Tensorflow debug logging\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# noinspection PyPep8Naming\n",
        "# import keras.backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from keras.constraints import Constraint\n",
        "# from keras.layers import Dense\n",
        "# from keras.layers import Input\n",
        "# from keras.layers import LSTM\n",
        "# from keras.layers import Lambda\n",
        "# from keras.models import Model\n",
        "\n",
        "# See https://github.com/OxCGRT/covid-policy-tracker\n",
        "DATA_URL = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv\"\n",
        "\n",
        "# ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) # CHANGE_FOR_SANDBOX\n",
        "ROOT_DIR = os.path.abspath('') # For colab and ipynb\n",
        "DATA_PATH = os.path.join(ROOT_DIR, 'data')\n",
        "DATA_FILE_PATH = os.path.join(DATA_PATH, 'OxCGRT_latest.csv')\n",
        "ADDITIONAL_CONTEXT_FILE = os.path.join(DATA_PATH, \"Additional_Context_Data_Global.csv\")\n",
        "ADDITIONAL_US_STATES_CONTEXT = os.path.join(DATA_PATH, \"US_states_populations.csv\")\n",
        "ADDITIONAL_UK_CONTEXT = os.path.join(DATA_PATH, \"uk_populations.csv\")\n",
        "ADDITIONAL_BRAZIL_CONTEXT = os.path.join(DATA_PATH, \"brazil_populations.csv\")\n",
        "\n",
        "NPI_COLUMNS = ['C1_School closing',\n",
        "               'C2_Workplace closing',\n",
        "               'C3_Cancel public events',\n",
        "               'C4_Restrictions on gatherings',\n",
        "               'C5_Close public transport',\n",
        "               'C6_Stay at home requirements',\n",
        "               'C7_Restrictions on internal movement',\n",
        "               'C8_International travel controls',\n",
        "               'H1_Public information campaigns',\n",
        "               'H2_Testing policy',\n",
        "               'H3_Contact tracing',\n",
        "               'H6_Facial Coverings']\n",
        "\n",
        "CONTEXT_COLUMNS = ['CountryName',\n",
        "                   'RegionName',\n",
        "                   'GeoID',\n",
        "                   'Date',\n",
        "                   'ConfirmedCases',\n",
        "                   'ConfirmedDeaths',\n",
        "                   'Population']\n",
        "NB_LOOKBACK_DAYS = 21\n",
        "NB_TEST_DAYS = 14\n",
        "WINDOW_SIZE = 7\n",
        "US_PREFIX = \"United States / \"\n",
        "NUM_TRIALS = 1\n",
        "LSTM_SIZE = 32\n",
        "MAX_NB_COUNTRIES = 20\n",
        "\n",
        "\n",
        "# Functions to be used for lambda layers in model\n",
        "def _combine_r_and_d(x):\n",
        "    r, d = x\n",
        "    return r * (1. - d)\n",
        "\n",
        "\n",
        "class XPrizePredictor(object):\n",
        "    \"\"\"\n",
        "    A class that computes a fitness for Prescriptor candidates.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path_to_model_weights, data_url, args = None):\n",
        "        self.args = args if args else MockArgs()\n",
        "\n",
        "        if path_to_model_weights:\n",
        "\n",
        "            # Load model weights\n",
        "            nb_context = 1  # Only time series of new cases rate is used as context\n",
        "            nb_action = len(NPI_COLUMNS)\n",
        "            self.predictor = self._construct_model(nb_context=nb_context,\n",
        "                                                      nb_action=nb_action,\n",
        "                                                      lstm_size=self.args.hidden_dim,\n",
        "                                                      nb_lookback_days=self.args.nb_lookback_days)\n",
        "            self.predictor.load_state_dict(torch.load(path_to_model_weights))\n",
        "            self.predictor.eval()\n",
        "\n",
        "            # Make sure data is available to make predictions\n",
        "            if not os.path.exists(DATA_FILE_PATH):\n",
        "                urllib.request.urlretrieve(DATA_URL, DATA_FILE_PATH)\n",
        "\n",
        "        self.df = self._prepare_dataframe(data_url)\n",
        "        geos = self.df.GeoID.unique()\n",
        "        self.country_samples = self._create_country_samples(self.df, geos, self.args)\n",
        "        \n",
        "    def predict(self,\n",
        "                start_date_str: str,\n",
        "                end_date_str: str,\n",
        "                path_to_ips_file: str) -> pd.DataFrame:\n",
        "        start_date = pd.to_datetime(start_date_str, format='%Y-%m-%d')\n",
        "        end_date = pd.to_datetime(end_date_str, format='%Y-%m-%d')\n",
        "        nb_days = (end_date - start_date).days + 1\n",
        "\n",
        "        # Load the npis into a DataFrame, handling regions\n",
        "        npis_df = self._load_original_data(path_to_ips_file)\n",
        "\n",
        "        # Prepare the output\n",
        "        forecast = {\"CountryName\": [],\n",
        "                    \"RegionName\": [],\n",
        "                    \"Date\": [],\n",
        "                    \"PredictedDailyNewCases\": []}\n",
        "\n",
        "        # For each requested geo\n",
        "        geos = npis_df.GeoID.unique()\n",
        "        for g in geos:\n",
        "            cdf = self.df[self.df.GeoID == g]\n",
        "            if len(cdf) == 0:\n",
        "                # we don't have historical data for this geo: return zeroes\n",
        "                pred_new_cases = [0] * nb_days\n",
        "                geo_start_date = start_date\n",
        "            else:\n",
        "                last_known_date = cdf.Date.max()\n",
        "                # Start predicting from start_date, unless there's a gap since last known date\n",
        "                geo_start_date = min(last_known_date + np.timedelta64(1, 'D'), start_date)\n",
        "                npis_gdf = npis_df[(npis_df.Date >= geo_start_date) & (npis_df.Date <= end_date)]\n",
        "\n",
        "                pred_new_cases = self._get_new_cases_preds(cdf, g, npis_gdf)\n",
        "\n",
        "            # Append forecast data to results to return\n",
        "            country = npis_df[npis_df.GeoID == g].iloc[0].CountryName\n",
        "            region = npis_df[npis_df.GeoID == g].iloc[0].RegionName\n",
        "            for i, pred in enumerate(pred_new_cases):\n",
        "                forecast[\"CountryName\"].append(country)\n",
        "                forecast[\"RegionName\"].append(region)\n",
        "                current_date = geo_start_date + pd.offsets.Day(i)\n",
        "                forecast[\"Date\"].append(current_date)\n",
        "                forecast[\"PredictedDailyNewCases\"].append(pred)\n",
        "\n",
        "        forecast_df = pd.DataFrame.from_dict(forecast)\n",
        "        # Return only the requested predictions\n",
        "        return forecast_df[(forecast_df.Date >= start_date) & (forecast_df.Date <= end_date)]\n",
        "\n",
        "    def _get_new_cases_preds(self, c_df, g, npis_df):\n",
        "        cdf = c_df[c_df.ConfirmedCases.notnull()]\n",
        "        initial_context_input = self.country_samples[g]['X_test_context'][-1]\n",
        "        initial_action_input = self.country_samples[g]['X_test_action'][-1]\n",
        "        # Predictions with passed npis\n",
        "        cnpis_df = npis_df[npis_df.GeoID == g]\n",
        "        npis_sequence = np.array(cnpis_df[NPI_COLUMNS])\n",
        "        # Get the predictions with the passed NPIs\n",
        "        self.predictor.eval()\n",
        "        preds = self._roll_out_predictions(self.predictor,\n",
        "                                           initial_context_input,\n",
        "                                           initial_action_input,\n",
        "                                           npis_sequence)\n",
        "        # Gather info to convert to total cases\n",
        "        prev_confirmed_cases = np.array(cdf.ConfirmedCases)\n",
        "        prev_new_cases = np.array(cdf.NewCases)\n",
        "        initial_total_cases = prev_confirmed_cases[-1]\n",
        "        pop_size = np.array(cdf.Population)[-1]  # Population size doesn't change over time\n",
        "        # Compute predictor's forecast\n",
        "        pred_new_cases = self._convert_ratios_to_total_cases(\n",
        "            preds,\n",
        "            self.args.window_size,\n",
        "            prev_new_cases,\n",
        "            initial_total_cases,\n",
        "            pop_size)\n",
        "\n",
        "        return pred_new_cases\n",
        "\n",
        "    def _prepare_dataframe(self, data_url: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Loads the Oxford dataset, cleans it up and prepares the necessary columns. Depending on options, also\n",
        "        loads the Johns Hopkins dataset and merges that in.\n",
        "        :param data_url: the url containing the original data\n",
        "        :return: a Pandas DataFrame with the historical data\n",
        "        \"\"\"\n",
        "        # Original df from Oxford\n",
        "        df1 = self._load_original_data(data_url)\n",
        "\n",
        "        # Additional context df (e.g Population for each country)\n",
        "        df2 = self._load_additional_context_df()\n",
        "\n",
        "        # Merge the 2 DataFrames\n",
        "        df = df1.merge(df2, on=['GeoID'], how='left', suffixes=('', '_y'))\n",
        "\n",
        "        # Drop countries with no population data\n",
        "        df.dropna(subset=['Population'], inplace=True)\n",
        "\n",
        "        #  Keep only needed columns\n",
        "        columns = CONTEXT_COLUMNS + NPI_COLUMNS\n",
        "        df = df[columns]\n",
        "\n",
        "        # Fill in missing values\n",
        "        self._fill_missing_values(df)\n",
        "\n",
        "        # Compute number of new cases and deaths each day\n",
        "        df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)\n",
        "        df['NewDeaths'] = df.groupby('GeoID').ConfirmedDeaths.diff().fillna(0)\n",
        "\n",
        "        # Replace negative values (which do not make sense for these columns) with 0\n",
        "        df['NewCases'] = df['NewCases'].clip(lower=0)\n",
        "        df['NewDeaths'] = df['NewDeaths'].clip(lower=0)\n",
        "\n",
        "        # Compute smoothed versions of new cases and deaths each day\n",
        "        df['SmoothNewCases'] = df.groupby('GeoID')['NewCases'].rolling(\n",
        "            self.args.window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
        "        df['SmoothNewDeaths'] = df.groupby('GeoID')['NewDeaths'].rolling(\n",
        "            self.args.window_size, center=False).mean().fillna(0).reset_index(0, drop=True)\n",
        "\n",
        "        # Compute percent change in new cases and deaths each day\n",
        "        df['CaseRatio'] = df.groupby('GeoID').SmoothNewCases.pct_change(\n",
        "        ).fillna(0).replace(np.inf, 0) + 1\n",
        "        df['DeathRatio'] = df.groupby('GeoID').SmoothNewDeaths.pct_change(\n",
        "        ).fillna(0).replace(np.inf, 0) + 1\n",
        "\n",
        "        # Add column for proportion of population infected\n",
        "        df['ProportionInfected'] = df['ConfirmedCases'] / df['Population']\n",
        "\n",
        "        # Create column of value to predict\n",
        "        df['PredictionRatio'] = df['CaseRatio'] / (1 - df['ProportionInfected'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_original_data(data_url):\n",
        "        latest_df = pd.read_csv(data_url,\n",
        "                                parse_dates=['Date'],\n",
        "                                encoding=\"ISO-8859-1\",\n",
        "                                dtype={\"RegionName\": str,\n",
        "                                       \"RegionCode\": str},\n",
        "                                error_bad_lines=False)\n",
        "        # GeoID is CountryName / RegionName\n",
        "        # np.where usage: if A then B else C\n",
        "        latest_df[\"GeoID\"] = np.where(latest_df[\"RegionName\"].isnull(),\n",
        "                                      latest_df[\"CountryName\"],\n",
        "                                      latest_df[\"CountryName\"] + ' / ' + latest_df[\"RegionName\"])\n",
        "        return latest_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _fill_missing_values(df):\n",
        "        \"\"\"\n",
        "        # Fill missing values by interpolation, ffill, and filling NaNs\n",
        "        :param df: Dataframe to be filled\n",
        "        \"\"\"\n",
        "        df.update(df.groupby('GeoID').ConfirmedCases.apply(\n",
        "            lambda group: group.interpolate(limit_area='inside')))\n",
        "        # Drop country / regions for which no number of cases is available\n",
        "        df.dropna(subset=['ConfirmedCases'], inplace=True)\n",
        "        df.update(df.groupby('GeoID').ConfirmedDeaths.apply(\n",
        "            lambda group: group.interpolate(limit_area='inside')))\n",
        "        # Drop country / regions for which no number of deaths is available\n",
        "        df.dropna(subset=['ConfirmedDeaths'], inplace=True)\n",
        "        for npi_column in NPI_COLUMNS:\n",
        "            df.update(df.groupby('GeoID')[npi_column].ffill().fillna(0))\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_additional_context_df():\n",
        "        # File containing the population for each country\n",
        "        # Note: this file contains only countries population, not regions\n",
        "        additional_context_df = pd.read_csv(ADDITIONAL_CONTEXT_FILE,\n",
        "                                            usecols=['CountryName', 'Population'])\n",
        "        additional_context_df['GeoID'] = additional_context_df['CountryName']\n",
        "\n",
        "        # US states population\n",
        "        additional_us_states_df = pd.read_csv(ADDITIONAL_US_STATES_CONTEXT,\n",
        "                                              usecols=['NAME', 'POPESTIMATE2019'])\n",
        "        # Rename the columns to match measures_df ones\n",
        "        additional_us_states_df.rename(columns={'POPESTIMATE2019': 'Population'}, inplace=True)\n",
        "        # Prefix with country name to match measures_df\n",
        "        additional_us_states_df['GeoID'] = US_PREFIX + additional_us_states_df['NAME']\n",
        "\n",
        "        # Append the new data to additional_df\n",
        "        additional_context_df = additional_context_df.append(additional_us_states_df)\n",
        "\n",
        "        # UK population\n",
        "        additional_uk_df = pd.read_csv(ADDITIONAL_UK_CONTEXT)\n",
        "        # Append the new data to additional_df\n",
        "        additional_context_df = additional_context_df.append(additional_uk_df)\n",
        "\n",
        "        # Brazil population\n",
        "        additional_brazil_df = pd.read_csv(ADDITIONAL_BRAZIL_CONTEXT)\n",
        "        # Append the new data to additional_df\n",
        "        additional_context_df = additional_context_df.append(additional_brazil_df)\n",
        "\n",
        "        return additional_context_df\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_country_samples(df: pd.DataFrame, geos: list, args: MockArgs) -> dict:\n",
        "        \"\"\"\n",
        "        For each country, creates numpy arrays for Keras\n",
        "        :param df: a Pandas DataFrame with historical data for countries (the \"Oxford\" dataset)\n",
        "        :param geos: a list of geo names\n",
        "        :return: a dictionary of train and test sets, for each specified country\n",
        "        \"\"\"\n",
        "        context_column = 'PredictionRatio'\n",
        "        action_columns = NPI_COLUMNS\n",
        "        outcome_column = 'PredictionRatio'\n",
        "        country_samples = {}\n",
        "        for g in geos:\n",
        "            cdf = df[df.GeoID == g]\n",
        "            cdf = cdf[cdf.ConfirmedCases.notnull()]\n",
        "            context_data = np.array(cdf[context_column])\n",
        "            action_data = np.array(cdf[action_columns])\n",
        "            outcome_data = np.array(cdf[outcome_column])\n",
        "            context_samples = []\n",
        "            action_samples = []\n",
        "            outcome_samples = []\n",
        "            nb_total_days = outcome_data.shape[0]\n",
        "            for d in range(args.nb_lookback_days, nb_total_days):\n",
        "                context_samples.append(context_data[d - args.nb_lookback_days:d])\n",
        "                action_samples.append(action_data[d - args.nb_lookback_days:d])\n",
        "                outcome_samples.append(outcome_data[d])\n",
        "            if len(outcome_samples) > 0:\n",
        "                X_context = np.expand_dims(np.stack(context_samples, axis=0), axis=2)\n",
        "                X_action = np.stack(action_samples, axis=0)\n",
        "                y = np.stack(outcome_samples, axis=0)\n",
        "                country_samples[g] = {\n",
        "                    'X_context': X_context,\n",
        "                    'X_action': X_action,\n",
        "                    'y': y,\n",
        "                    'X_train_context': X_context[:-args.nb_test_days],\n",
        "                    'X_train_action': X_action[:-args.nb_test_days],\n",
        "                    'y_train': y[:-args.nb_test_days],\n",
        "                    'X_test_context': X_context[-args.nb_test_days:],\n",
        "                    'X_test_action': X_action[-args.nb_test_days:],\n",
        "                    'y_test': y[-args.nb_test_days:],\n",
        "                }\n",
        "        return country_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Function for performing roll outs into the future\n",
        "    # TODO debug with pytoch\n",
        "    @staticmethod\n",
        "    def _roll_out_predictions(predictor, initial_context_input, initial_action_input, future_action_sequence):\n",
        "        nb_roll_out_days = future_action_sequence.shape[0]\n",
        "        pred_output = np.zeros(nb_roll_out_days)\n",
        "        context_input = np.expand_dims(np.copy(initial_context_input), axis=0) #\n",
        "        action_input = np.expand_dims(np.copy(initial_action_input), axis=0)\n",
        "\n",
        "        for d in range(nb_roll_out_days):\n",
        "            action_input[:, :-1] = action_input[:, 1:]\n",
        "            # Use the passed actions\n",
        "            action_sequence = future_action_sequence[d]\n",
        "            action_input[:, -1] = action_sequence\n",
        "            pred = predictor.predict([context_input, action_input]) #TODO fix this for pytorch outputs model(input) (once model has een trained)\n",
        "            pred_output[d] = pred\n",
        "            context_input[:, :-1] = context_input[:, 1:]\n",
        "            context_input[:, -1] = pred\n",
        "        return pred_output\n",
        "\n",
        "    # Functions for converting predictions back to number of cases\n",
        "    @staticmethod\n",
        "    def _convert_ratio_to_new_cases(ratio,\n",
        "                                    window_size,\n",
        "                                    prev_new_cases_list,\n",
        "                                    prev_pct_infected):\n",
        "        return (ratio * (1 - prev_pct_infected) - 1) * \\\n",
        "               (window_size * np.mean(prev_new_cases_list[-window_size:])) \\\n",
        "               + prev_new_cases_list[-window_size]\n",
        "\n",
        "    def _convert_ratios_to_total_cases(self,\n",
        "                                       ratios,\n",
        "                                       window_size,\n",
        "                                       prev_new_cases,\n",
        "                                       initial_total_cases,\n",
        "                                       pop_size):\n",
        "        new_new_cases = []\n",
        "        prev_new_cases_list = list(prev_new_cases)\n",
        "        curr_total_cases = initial_total_cases\n",
        "        for ratio in ratios:\n",
        "            new_cases = self._convert_ratio_to_new_cases(ratio,\n",
        "                                                         window_size,\n",
        "                                                         prev_new_cases_list,\n",
        "                                                         curr_total_cases / pop_size)\n",
        "            # new_cases can't be negative!\n",
        "            new_cases = max(0, new_cases)\n",
        "            # Which means total case/s can't go down\n",
        "            curr_total_cases += new_cases\n",
        "            # Update prev_new_cases_list for next iteration of the loop\n",
        "            prev_new_cases_list.append(new_cases)\n",
        "            new_new_cases.append(new_cases)\n",
        "        return new_new_cases\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_case_list(case_list, window):\n",
        "        return pd.Series(case_list).rolling(window).mean().to_numpy()\n",
        "\n",
        "    def train(self, skip_training = False):\n",
        "        print(\"Creating numpy arrays for Keras for each country...\")\n",
        "        geos = self._most_affected_geos(self.df, self.args.max_nb_countries, self.args.nb_lookback_days)\n",
        "        country_samples = self._create_country_samples(self.df, geos, self.args)\n",
        "        print(\"Numpy arrays created\")\n",
        "\n",
        "        # Aggregate data for training\n",
        "        all_X_context_list = [country_samples[c]['X_train_context']\n",
        "                              for c in country_samples]\n",
        "        all_X_action_list = [country_samples[c]['X_train_action']\n",
        "                             for c in country_samples]\n",
        "        all_y_list = [country_samples[c]['y_train']\n",
        "                      for c in country_samples]\n",
        "        X_context = np.concatenate(all_X_context_list)\n",
        "        X_action = np.concatenate(all_X_action_list)\n",
        "        y = np.concatenate(all_y_list)\n",
        "\n",
        "        # Clip outliers\n",
        "        MIN_VALUE = 0.\n",
        "        MAX_VALUE = 2.\n",
        "        X_context = np.clip(X_context, MIN_VALUE, MAX_VALUE)\n",
        "        y = np.clip(y, MIN_VALUE, MAX_VALUE)\n",
        "\n",
        "        # Aggregate data for testing only on top countries\n",
        "        test_all_X_context_list = [country_samples[g]['X_train_context']\n",
        "                                   for g in geos]\n",
        "        test_all_X_action_list = [country_samples[g]['X_train_action']\n",
        "                                  for g in geos]\n",
        "        test_all_y_list = [country_samples[g]['y_train']\n",
        "                           for g in geos]\n",
        "        test_X_context = np.concatenate(test_all_X_context_list)\n",
        "        test_X_action = np.concatenate(test_all_X_action_list)\n",
        "        test_y = np.concatenate(test_all_y_list)\n",
        "\n",
        "        test_X_context = np.clip(test_X_context, MIN_VALUE, MAX_VALUE)\n",
        "        test_y = np.clip(test_y, MIN_VALUE, MAX_VALUE)\n",
        "\n",
        "\n",
        "\n",
        "        # Run full training several times to find best model\n",
        "        # and gather data for setting acceptance threshold\n",
        "        models = [] if not skip_training else [self.predictor]\n",
        "        if not skip_training:\n",
        "            train_losses = []\n",
        "            val_losses = []\n",
        "            test_losses = []\n",
        "            for t in range(self.args.num_trials):\n",
        "                print('Trial', t)\n",
        "\n",
        "                # X_context, X_action, y = self._permute_data(X_context, X_action, y, seed=t)\n",
        "                \n",
        "                model = self._construct_model(\n",
        "                    nb_context=X_context.shape[-1],\n",
        "                    nb_action=X_action.shape[-1],\n",
        "                    lstm_size=self.args.hidden_dim,\n",
        "                    nb_lookback_days=self.args.nb_lookback_days\n",
        "                )\n",
        "                run(self.args, model, X_context, X_action, y, t)\n",
        "                models.append(model)\n",
        "\n",
        "        # Gather test info\n",
        "        country_indeps = []\n",
        "        country_predss = []\n",
        "        country_casess = []\n",
        "        for model in models: # TODO update rollouts to pytorch\n",
        "            country_indep, country_preds, country_cases = self._lstm_get_test_rollouts(model,\n",
        "                                                                                       self.df,\n",
        "                                                                                       geos,\n",
        "                                                                                       country_samples)\n",
        "            country_indeps.append(country_indep)\n",
        "            country_predss.append(country_preds)\n",
        "            country_casess.append(country_cases)\n",
        "\n",
        "        # Compute cases mae\n",
        "        test_case_maes = []\n",
        "        for m in range(len(models)):\n",
        "            total_loss = 0\n",
        "            for g in geos:\n",
        "                true_cases = np.sum(np.array(self.df[self.df.GeoID == g].NewCases)[-self.args.nb_test_days:])\n",
        "                pred_cases = np.sum(country_casess[m][g][-self.args.nb_test_days:])\n",
        "                total_loss += np.abs(true_cases - pred_cases)\n",
        "            test_case_maes.append(total_loss)\n",
        "\n",
        "        # Select best model\n",
        "        print(f\"MAE per model: {test_case_maes}\")\n",
        "        best_model = models[np.argmin(test_case_maes)]\n",
        "        print(f\"best model was number {np.argmin(test_case_maes)}\")\n",
        "        for t in range(self.args.num_trials):\n",
        "            logdir = args.log_path + args.model_name + f\"_{t}/\"\n",
        "            writer = SummaryWriter(logdir)\n",
        "            for i, mae in enumerate(test_case_maes):\n",
        "                writer.add_scalar(\"Xprize/test/mae\", mae, i)\n",
        "\n",
        "        self.predictor = best_model\n",
        "        torch.save(best_model.state_dict(), f\"{self.args.output}{self.args.model_name}.pth\")\n",
        "        print(\"Done\")\n",
        "        return best_model\n",
        "\n",
        "    @staticmethod\n",
        "    def _most_affected_geos(df, nb_geos, min_historical_days):\n",
        "        \"\"\"\n",
        "        Returns the list of most affected countries, in terms of confirmed deaths.\n",
        "        :param df: the data frame containing the historical data\n",
        "        :param nb_geos: the number of geos to return\n",
        "        :param min_historical_days: the minimum days of historical data the countries must have\n",
        "        :return: a list of country names of size nb_countries if there were enough, and otherwise a list of all the\n",
        "        country names that have at least min_look_back_days data points.\n",
        "        \"\"\"\n",
        "        # By default use most affected geos with enough history\n",
        "        gdf = df.groupby('GeoID')['ConfirmedDeaths'].agg(['max', 'count']).sort_values(by='max', ascending=False)\n",
        "        filtered_gdf = gdf[gdf[\"count\"] > min_historical_days]\n",
        "        geos = list(filtered_gdf.head(nb_geos).index)\n",
        "        return geos\n",
        "\n",
        "    # Shuffling data prior to train/val split\n",
        "    def _permute_data(self, X_context, X_action, y, seed=301):\n",
        "        np.random.seed(seed)\n",
        "        p = np.random.permutation(y.shape[0])\n",
        "        X_context = X_context[p]\n",
        "        X_action = X_action[p]\n",
        "        y = y[p]\n",
        "        return X_context, X_action, y\n",
        "\n",
        "    # Construct model\n",
        "    def _construct_model(self, nb_context, nb_action, lstm_size=32, nb_lookback_days=21):\n",
        "        model = CombinedModel(nb_lookback_days, nb_action, nb_context, lstm_size, self.args)\n",
        "        return model\n",
        "\n",
        "    # Train model\n",
        "    def _train_model(self, training_model, X_context, X_action, y, epochs=1, verbose=0):\n",
        "        pass\n",
        "\n",
        "    # Functions for computing test metrics\n",
        "    def _lstm_roll_out_predictions(self, model, initial_context_input, initial_action_input, future_action_sequence):\n",
        "        nb_test_days = future_action_sequence.shape[0]\n",
        "        pred_output = np.zeros(nb_test_days)\n",
        "        context_input = np.expand_dims(np.copy(initial_context_input), axis=0)\n",
        "        action_input = np.expand_dims(np.copy(initial_action_input), axis=0) # TODO make a dataset out of these using XPrizeDataloader see the pytorch train() and run() methods\n",
        "        for d in range(nb_test_days):\n",
        "            action_input[:, :-1] = action_input[:, 1:]\n",
        "            action_input[:, -1] = future_action_sequence[d]\n",
        "\n",
        "            pred = model.predict([context_input, action_input])\n",
        "            pred_output[d] = pred\n",
        "            context_input[:, :-1] = context_input[:, 1:]\n",
        "            context_input[:, -1] = pred\n",
        "        return pred_output\n",
        "\n",
        "    def _lstm_get_test_rollouts(self, model, df, top_geos, country_samples):\n",
        "        country_indep = {}\n",
        "        country_preds = {}\n",
        "        country_cases = {}\n",
        "        for g in top_geos:\n",
        "            X_test_context = country_samples[g]['X_test_context']\n",
        "            X_test_action = country_samples[g]['X_test_action']\n",
        "            country_indep[g] = model.predict([X_test_context, X_test_action]) # TODO make a dataset out of these using XPrizeDataloader see the pytorch train() and run() methods\n",
        "\n",
        "            initial_context_input = country_samples[g]['X_test_context'][0]\n",
        "            initial_action_input = country_samples[g]['X_test_action'][0]\n",
        "            y_test = country_samples[g]['y_test']\n",
        "\n",
        "            nb_test_days = y_test.shape[0]\n",
        "            nb_actions = initial_action_input.shape[-1]\n",
        "\n",
        "            future_action_sequence = np.zeros((nb_test_days, nb_actions))\n",
        "            future_action_sequence[:nb_test_days] = country_samples[g]['X_test_action'][:, -1, :]\n",
        "            current_action = country_samples[g]['X_test_action'][:, -1, :][-1]\n",
        "            future_action_sequence[14:] = current_action\n",
        "            preds = self._lstm_roll_out_predictions(model,\n",
        "                                                    initial_context_input,\n",
        "                                                    initial_action_input,\n",
        "                                                    future_action_sequence)\n",
        "            country_preds[g] = preds\n",
        "\n",
        "            prev_confirmed_cases = np.array(\n",
        "                df[df.GeoID == g].ConfirmedCases)[:-nb_test_days]\n",
        "            prev_new_cases = np.array(\n",
        "                df[df.GeoID == g].NewCases)[:-nb_test_days]\n",
        "            initial_total_cases = prev_confirmed_cases[-1]\n",
        "            pop_size = np.array(df[df.GeoID == g].Population)[0]\n",
        "\n",
        "            pred_new_cases = self._convert_ratios_to_total_cases(\n",
        "                preds, self.args.window_size, prev_new_cases, initial_total_cases, pop_size)\n",
        "            country_cases[g] = pred_new_cases\n",
        "        return country_indep, country_preds, country_cases\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX4jqqAENnom"
      },
      "source": [
        "# MAIN\n",
        "To train new models run the first two predictor lines\n",
        "```\n",
        "predictor = XPrizePredictor(None, DATA_URL, args = args)\n",
        "predictor_model = predictor.train()\n",
        "```\n",
        "\n",
        "\n",
        "To load an existing model run the second two\n",
        "```\n",
        "predictor = XPrizePredictor(f\"./modelsaves/{args.model_name}.pth\", DATA_URL, args = args)\n",
        "predictor_model = predictor.train(skip_training=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToMoYbKMrLMZ",
        "outputId": "ec21b8c9-f0a8-4381-9eb1-6b61aae5e9bb"
      },
      "source": [
        "# This block was used for hyperparameter sweep. FOllow these examples for tensorbaord or skip to the prediction section.\n",
        "\n",
        "VERSION_NUM = 12\n",
        "\n",
        "block_criterion = \"l1\"\n",
        "\n",
        "args = MockArgs()\n",
        "# args.batch_size = 50\n",
        "args.dry_run = False\n",
        "# args.epochs = 20\n",
        "# args.learning_rate = .003\n",
        "# args.num_trials = 5\n",
        "args.optimizer= \"adam\"\n",
        "args.criterion= block_criterion\n",
        "args.flush_history = False\n",
        "args.xprize_lambda = 1e-6\n",
        "args.set_regularizers()\n",
        "args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}\"\n",
        "\n",
        "# model_path = f'{args.output}{args.model_name}.pth'\n",
        "# predictor = XPrizePredictor(None, DATA_URL, args = args)\n",
        "# predictor_model = predictor.train() # TERRIBLE with ratio .5\n",
        "\n",
        "RUN_NUM = \"2xa-K.15\"\n",
        "\n",
        "\n",
        "args.hidden_dim= 32\n",
        "args.hidden_dim_action= 64\n",
        "args.xprize_regularizer = True\n",
        "args.set_regularizers()\n",
        "args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}\"\n",
        "\n",
        "# model_path = f'{args.output}{args.model_name}.pth'\n",
        "# predictor2 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "# predictor_model2 = predictor2.train() # pretty steady, 2.4 million mae with ratio .5\n",
        "\n",
        "RUN_NUM = \"2xca-K.15\"\n",
        "\n",
        "\n",
        "args.hidden_dim= 64\n",
        "args.hidden_dim_action= 64\n",
        "args.xprize_regularizer = True\n",
        "args.set_regularizers()\n",
        "args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}\"\n",
        "\n",
        "# model_path = f'{args.output}{args.model_name}.pth'\n",
        "# predictor2 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "# predictor_model2 = predictor2.train() # pretty steady, 2.4 million mae with ratio .5\n",
        "\n",
        "RUN_NUM = \"2xc4xa-K.15\"\n",
        "args.hidden_dim= 64\n",
        "args.hidden_dim_action= 128\n",
        "args.xprize_regularizer = True\n",
        "args.set_regularizers()\n",
        "args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}\"\n",
        "\n",
        "model_path = f'{args.output}{args.model_name}.pth'\n",
        "predictor2 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "predictor_model2 = predictor2.train() # pretty steady, 2.4 million mae with ratio .5\n",
        "\n",
        "\n",
        "# args.l2_regularizer=True\n",
        "# args.set_regularizers()\n",
        "# args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}_L2\"\n",
        "\n",
        "# model_path = f'{args.output}{args.model_name}.pth'\n",
        "# predictor3 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "# predictor_model3 = predictor3.train() # TERRIBLE with ratio .5\n",
        "\n",
        "\n",
        "# args.orth_regularizer=True\n",
        "# args.set_regularizers()\n",
        "# args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}_L2_ORTH\"\n",
        "\n",
        "# model_path = f'{args.output}{args.model_name}.pth'\n",
        "# predictor4 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "# predictor_model4 = predictor4.train() # 2.2 million mae took last epoch\n",
        "\n",
        "# predictor = XPrizePredictor(f\"./modelsaves/{args.model_name}.pth\", DATA_URL, args = args)\n",
        "# predictor_model = predictor.train(skip_training=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating numpy arrays for Keras for each country...\n",
            "Numpy arrays created\n",
            "Trial 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 18/18 [00:09<00:00,  1.92it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 0.8277 | val_loss: 0.1517 | val rmse: 0.1454 | val mean_absolute_error: 0.0707 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.97it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.0967 | val_loss: 0.0728 | val rmse: 0.1463 | val mean_absolute_error: 0.0708 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.99it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0729 | val_loss: 0.0689 | val rmse: 0.1458 | val mean_absolute_error: 0.0706 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.92it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0690 | val_loss: 0.0695 | val rmse: 0.1453 | val mean_absolute_error: 0.0707 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.97it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0691 | val_loss: 0.0681 | val rmse: 0.1433 | val mean_absolute_error: 0.0700 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0696 | val_loss: 0.0687 | val rmse: 0.1456 | val mean_absolute_error: 0.0707 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.96it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0689 | val_loss: 0.0687 | val rmse: 0.1439 | val mean_absolute_error: 0.0707 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0686 | val_loss: 0.0691 | val rmse: 0.1457 | val mean_absolute_error: 0.0712 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0696 | val_loss: 0.0686 | val rmse: 0.1446 | val mean_absolute_error: 0.0708 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "Stop training at epoch 8. The lowest loss achieved is 0.06856683434711562 at epoch 4\n",
            "Trial 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.6771 | val_loss: 0.2148 | val rmse: 0.1411 | val mean_absolute_error: 0.0689 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.1741 | val_loss: 0.0985 | val rmse: 0.1442 | val mean_absolute_error: 0.0695 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0852 | val_loss: 0.0682 | val rmse: 0.1423 | val mean_absolute_error: 0.0698 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0700 | val_loss: 0.0684 | val rmse: 0.1429 | val mean_absolute_error: 0.0693 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0689 | val_loss: 0.0680 | val rmse: 0.1443 | val mean_absolute_error: 0.0698 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.96it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0678 | val_loss: 0.0678 | val rmse: 0.1444 | val mean_absolute_error: 0.0698 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.97it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0675 | val_loss: 0.0682 | val rmse: 0.1461 | val mean_absolute_error: 0.0704 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0673 | val_loss: 0.0678 | val rmse: 0.1449 | val mean_absolute_error: 0.0697 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0682 | val_loss: 0.0674 | val rmse: 0.1432 | val mean_absolute_error: 0.0697 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0682 | val_loss: 0.0686 | val rmse: 0.1444 | val mean_absolute_error: 0.0704 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.90it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0690 | val_loss: 0.0685 | val rmse: 0.1445 | val mean_absolute_error: 0.0696 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 12 / 17] train_loss: 0.0680 | val_loss: 0.0678 | val rmse: 0.1437 | val mean_absolute_error: 0.0700 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 13 / 17] train_loss: 0.0686 | val_loss: 0.0703 | val rmse: 0.1436 | val mean_absolute_error: 0.0699 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "Stop training at epoch 12. The lowest loss achieved is 0.07032734817928737 at epoch 8\n",
            "Trial 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.3923 | val_loss: 0.1781 | val rmse: 0.1489 | val mean_absolute_error: 0.0713 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.1457 | val_loss: 0.1085 | val rmse: 0.1486 | val mean_absolute_error: 0.0716 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0903 | val_loss: 0.0819 | val rmse: 0.1490 | val mean_absolute_error: 0.0720 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0724 | val_loss: 0.0696 | val rmse: 0.1484 | val mean_absolute_error: 0.0715 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0700 | val_loss: 0.0693 | val rmse: 0.1494 | val mean_absolute_error: 0.0716 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0696 | val_loss: 0.0688 | val rmse: 0.1472 | val mean_absolute_error: 0.0707 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0697 | val_loss: 0.0692 | val rmse: 0.1480 | val mean_absolute_error: 0.0714 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.95it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0690 | val_loss: 0.0703 | val rmse: 0.1477 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0692 | val_loss: 0.0686 | val rmse: 0.1451 | val mean_absolute_error: 0.0705 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.93it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0694 | val_loss: 0.0698 | val rmse: 0.1488 | val mean_absolute_error: 0.0715 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.90it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0702 | val_loss: 0.0688 | val rmse: 0.1467 | val mean_absolute_error: 0.0710 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 12 / 17] train_loss: 0.0697 | val_loss: 0.0693 | val rmse: 0.1468 | val mean_absolute_error: 0.0715 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:09<00:00,  1.94it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 13 / 17] train_loss: 0.0696 | val_loss: 0.0692 | val rmse: 0.1466 | val mean_absolute_error: 0.0713 | val explained_variance: -0.0000\n",
            "==================================================\n",
            "Stop training at epoch 12. The lowest loss achieved is 0.06919974730246597 at epoch 8\n",
            "MAE per model: [2816290.3218064103, 4157846.5849052356, 2090085.088321695]\n",
            "best model was number 2\n",
            "Done\n",
            "Creating numpy arrays for Keras for each country...\n",
            "Numpy arrays created\n",
            "Trial 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 18/18 [00:09<00:00,  1.81it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.1590 | val_loss: 0.1709 | val rmse: 0.1451 | val mean_absolute_error: 0.0703 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.75it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.1278 | val_loss: 0.0976 | val rmse: 0.1501 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0792 | val_loss: 0.0777 | val rmse: 0.1476 | val mean_absolute_error: 0.0711 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.76it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0737 | val_loss: 0.0717 | val rmse: 0.1483 | val mean_absolute_error: 0.0709 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.75it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0718 | val_loss: 0.0741 | val rmse: 0.1482 | val mean_absolute_error: 0.0713 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.79it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0712 | val_loss: 0.0701 | val rmse: 0.1481 | val mean_absolute_error: 0.0709 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0700 | val_loss: 0.0697 | val rmse: 0.1465 | val mean_absolute_error: 0.0704 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0706 | val_loss: 0.0700 | val rmse: 0.1469 | val mean_absolute_error: 0.0706 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0705 | val_loss: 0.0716 | val rmse: 0.1494 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.80it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0706 | val_loss: 0.0709 | val rmse: 0.1503 | val mean_absolute_error: 0.0716 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0723 | val_loss: 0.0714 | val rmse: 0.1492 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "Stop training at epoch 10. The lowest loss achieved is 0.07139254361391068 at epoch 6\n",
            "Trial 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.3472 | val_loss: 0.1076 | val rmse: 0.1454 | val mean_absolute_error: 0.0713 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.1702 | val_loss: 0.1123 | val rmse: 0.1471 | val mean_absolute_error: 0.0716 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0908 | val_loss: 0.0735 | val rmse: 0.1475 | val mean_absolute_error: 0.0718 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0741 | val_loss: 0.0702 | val rmse: 0.1465 | val mean_absolute_error: 0.0711 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0711 | val_loss: 0.0704 | val rmse: 0.1459 | val mean_absolute_error: 0.0712 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0700 | val_loss: 0.0716 | val rmse: 0.1476 | val mean_absolute_error: 0.0717 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0711 | val_loss: 0.0706 | val rmse: 0.1476 | val mean_absolute_error: 0.0718 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0705 | val_loss: 0.0704 | val rmse: 0.1461 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "Stop training at epoch 7. The lowest loss achieved is 0.07036312276290523 at epoch 3\n",
            "Trial 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 0.7233 | val_loss: 0.0774 | val rmse: 0.1450 | val mean_absolute_error: 0.0705 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.0966 | val_loss: 0.0774 | val rmse: 0.1472 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0752 | val_loss: 0.0728 | val rmse: 0.1455 | val mean_absolute_error: 0.0704 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0711 | val_loss: 0.0710 | val rmse: 0.1469 | val mean_absolute_error: 0.0711 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0695 | val_loss: 0.0701 | val rmse: 0.1445 | val mean_absolute_error: 0.0704 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0709 | val_loss: 0.0715 | val rmse: 0.1460 | val mean_absolute_error: 0.0708 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.76it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0711 | val_loss: 0.0711 | val rmse: 0.1461 | val mean_absolute_error: 0.0708 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.75it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0706 | val_loss: 0.0695 | val rmse: 0.1429 | val mean_absolute_error: 0.0700 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0705 | val_loss: 0.0703 | val rmse: 0.1478 | val mean_absolute_error: 0.0713 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.77it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0710 | val_loss: 0.0719 | val rmse: 0.1481 | val mean_absolute_error: 0.0716 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0705 | val_loss: 0.0698 | val rmse: 0.1452 | val mean_absolute_error: 0.0708 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.78it/s]\n",
            "100%|| 18/18 [00:02<00:00,  7.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 12 / 17] train_loss: 0.0707 | val_loss: 0.0738 | val rmse: 0.1476 | val mean_absolute_error: 0.0709 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "Stop training at epoch 11. The lowest loss achieved is 0.07382188116510709 at epoch 7\n",
            "MAE per model: [1627487.0607890007, 1428147.6382434994, 1697741.750663823]\n",
            "best model was number 1\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQpZWJr3yIhO",
        "outputId": "517fb452-967a-4cb7-a5aa-459a0ce909b8"
      },
      "source": [
        "RUN_NUM = \"4xca-K.15\"\n",
        "args.hidden_dim= 128\n",
        "args.hidden_dim_action= 128\n",
        "args.xprize_regularizer = True\n",
        "args.set_regularizers()\n",
        "args.model_name = f\"lstm_v{VERSION_NUM}-{RUN_NUM}_{args.optimizer}_{args.criterion}_lr_{str(args.learning_rate)[1:]}_batch_{args.batch_size}_XR_{args.xprize_lambda}\"\n",
        "\n",
        "\n",
        "model_path = f'{args.output}{args.model_name}.pth'\n",
        "predictor3 = XPrizePredictor(None, DATA_URL, args = args)\n",
        "predictor_model3 = predictor3.train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating numpy arrays for Keras for each country...\n",
            "Numpy arrays created\n",
            "Trial 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.6147 | val_loss: 0.0739 | val rmse: 0.1485 | val mean_absolute_error: 0.0719 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.1436 | val_loss: 0.1122 | val rmse: 0.1485 | val mean_absolute_error: 0.0718 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0838 | val_loss: 0.0784 | val rmse: 0.1496 | val mean_absolute_error: 0.0723 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0734 | val_loss: 0.0737 | val rmse: 0.1473 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0723 | val_loss: 0.0708 | val rmse: 0.1477 | val mean_absolute_error: 0.0720 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0717 | val_loss: 0.0700 | val rmse: 0.1473 | val mean_absolute_error: 0.0713 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0711 | val_loss: 0.0712 | val rmse: 0.1487 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.67it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0713 | val_loss: 0.0708 | val rmse: 0.1476 | val mean_absolute_error: 0.0720 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0712 | val_loss: 0.0704 | val rmse: 0.1487 | val mean_absolute_error: 0.0721 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0708 | val_loss: 0.0705 | val rmse: 0.1489 | val mean_absolute_error: 0.0722 | val explained_variance: -0.0000\n",
            "==================================================\n",
            "Stop training at epoch 9. The lowest loss achieved is 0.07054322771728039 at epoch 5\n",
            "Trial 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 3.7734 | val_loss: 1.0483 | val rmse: 1.0557 | val mean_absolute_error: 1.0460 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 4.2035 | val_loss: 1.0486 | val rmse: 1.0559 | val mean_absolute_error: 1.0462 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 4.4257 | val_loss: 1.0479 | val rmse: 1.0554 | val mean_absolute_error: 1.0457 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 4.7082 | val_loss: 1.0467 | val rmse: 1.0548 | val mean_absolute_error: 1.0453 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.62it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 4.9116 | val_loss: 1.0434 | val rmse: 1.0558 | val mean_absolute_error: 1.0463 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 4.0091 | val_loss: 1.5250 | val rmse: 1.9588 | val mean_absolute_error: 1.9538 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.62it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.5267 | val_loss: 0.2148 | val rmse: 0.1492 | val mean_absolute_error: 0.0724 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.2067 | val_loss: 0.1401 | val rmse: 0.1475 | val mean_absolute_error: 0.0715 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.62it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0922 | val_loss: 0.0845 | val rmse: 0.1490 | val mean_absolute_error: 0.0717 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0763 | val_loss: 0.0721 | val rmse: 0.1480 | val mean_absolute_error: 0.0716 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0721 | val_loss: 0.0718 | val rmse: 0.1483 | val mean_absolute_error: 0.0719 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 12 / 17] train_loss: 0.0716 | val_loss: 0.0703 | val rmse: 0.1477 | val mean_absolute_error: 0.0713 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 13 / 17] train_loss: 0.0707 | val_loss: 0.0711 | val rmse: 0.1478 | val mean_absolute_error: 0.0721 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 14 / 17] train_loss: 0.0713 | val_loss: 0.0711 | val rmse: 0.1477 | val mean_absolute_error: 0.0718 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 15 / 17] train_loss: 0.0724 | val_loss: 0.0709 | val rmse: 0.1486 | val mean_absolute_error: 0.0720 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.66it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 16 / 17] train_loss: 0.0713 | val_loss: 0.0712 | val rmse: 0.1495 | val mean_absolute_error: 0.0722 | val explained_variance: -0.0000\n",
            "==================================================\n",
            "Stop training at epoch 15. The lowest loss achieved is 0.07120429683062765 at epoch 11\n",
            "Trial 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 1 / 17] train_loss: 1.7100 | val_loss: 0.1275 | val rmse: 0.1468 | val mean_absolute_error: 0.0715 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 2 / 17] train_loss: 0.2000 | val_loss: 0.0733 | val rmse: 0.1500 | val mean_absolute_error: 0.0726 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 3 / 17] train_loss: 0.0859 | val_loss: 0.0756 | val rmse: 0.1469 | val mean_absolute_error: 0.0713 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 4 / 17] train_loss: 0.0725 | val_loss: 0.0722 | val rmse: 0.1491 | val mean_absolute_error: 0.0722 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 5 / 17] train_loss: 0.0706 | val_loss: 0.0734 | val rmse: 0.1492 | val mean_absolute_error: 0.0724 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 6 / 17] train_loss: 0.0714 | val_loss: 0.0732 | val rmse: 0.1485 | val mean_absolute_error: 0.0718 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 7 / 17] train_loss: 0.0734 | val_loss: 0.0718 | val rmse: 0.1492 | val mean_absolute_error: 0.0722 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.65it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 8 / 17] train_loss: 0.0725 | val_loss: 0.0712 | val rmse: 0.1490 | val mean_absolute_error: 0.0719 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.61it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 9 / 17] train_loss: 0.0706 | val_loss: 0.0718 | val rmse: 0.1467 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 10 / 17] train_loss: 0.0749 | val_loss: 0.0738 | val rmse: 0.1475 | val mean_absolute_error: 0.0715 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.60it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 11 / 17] train_loss: 0.0717 | val_loss: 0.0703 | val rmse: 0.1458 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 12 / 17] train_loss: 0.0713 | val_loss: 0.0708 | val rmse: 0.1488 | val mean_absolute_error: 0.0717 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 13 / 17] train_loss: 0.0727 | val_loss: 0.0702 | val rmse: 0.1460 | val mean_absolute_error: 0.0711 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 14 / 17] train_loss: 0.0705 | val_loss: 0.0715 | val rmse: 0.1491 | val mean_absolute_error: 0.0717 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 15 / 17] train_loss: 0.0700 | val_loss: 0.0702 | val rmse: 0.1479 | val mean_absolute_error: 0.0714 | val explained_variance: -0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:11<00:00,  1.63it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 16 / 17] train_loss: 0.0724 | val_loss: 0.0699 | val rmse: 0.1466 | val mean_absolute_error: 0.0709 | val explained_variance: 0.0000\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|| 18/18 [00:10<00:00,  1.64it/s]\n",
            "100%|| 18/18 [00:02<00:00,  6.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch: 17 / 17] train_loss: 0.0719 | val_loss: 0.0718 | val rmse: 0.1485 | val mean_absolute_error: 0.0719 | val explained_variance: 0.0000\n",
            "==================================================\n",
            "MAE per model: [1562964.7320169755, 1913905.0185998445, 1564491.0364312034]\n",
            "best model was number 0\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omuQs4YzLPZN"
      },
      "source": [
        "## Prediction\n",
        "To run predict you need to download another csv and add it to `/content/data/` (defualt directory is content). upload all files from [here](https://github.com/leaf-ai/covid-xprize/tree/master/covid_xprize/validation/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWMDMF6wKhr"
      },
      "source": [
        "\n",
        "NPIS_INPUT_FILE = \"./data/2020-09-30_historical_ip.csv\" # same as ./data\n",
        "start_date = \"2020-08-01\"\n",
        "end_date = \"2020-08-31\"\n",
        "\n",
        "# predictor = XPrizePredictor(f\"./modelsaves/{args.model_name}.pth\", DATA_URL, args = args)\n",
        "# predictor_model = predictor.train(skip_training=True)\n",
        "\n",
        "# Smooth l1 loss for the GRU has the best MAE so far: MAE per model: [2796938.8493539672]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqrWi608LGIj"
      },
      "source": [
        "Prediction timeit output: \n",
        "\n",
        "37.7 s  150 ms per loop (mean  std. dev. of 7 runs, 1 loop each)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NY1zfFaK8Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f305f06-d06a-4238-a506-eeae9d5ceaaf"
      },
      "source": [
        "# %%timeit\n",
        "predictor.predictor3 = predictor_model3\n",
        "new_predictor = CombinedModel(predictor3.predictor.nb_loopback_days, predictor3.predictor.nb_action_dim, predictor3.predictor.nb_context_dim, predictor3.predictor.hidden_dim, predictor3.predictor.args)\n",
        "new_predictor.load_state_dict(predictor3.predictor.state_dict())\n",
        "print(new_predictor)\n",
        "predictor3.predictor = new_predictor.eval()\n",
        "# predictor.compute_maes()\n",
        "# preds_df = predictor.predict(start_date, end_date, NPIS_INPUT_FILE)\n",
        "# preds_df = predictor2.predict(start_date, end_date, NPIS_INPUT_FILE)\n",
        "with torch.no_grad():\n",
        "    preds_df = predictor3.predict(start_date, end_date, NPIS_INPUT_FILE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CombinedModel(\n",
            "  (action_encoder): ActionEncoder(\n",
            "    (activation): Sigmoid()\n",
            "    (rnn): LSTM(12, 128)\n",
            "    (ln_rnn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (hidden2out): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            "  (context_encoder): ContextEncoder(\n",
            "    (activation): Softplus(beta=1, threshold=20)\n",
            "    (rnn): LSTM(1, 128)\n",
            "    (ln_rnn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    (hidden2out): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M4_qRbULEZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5c31558f-33ce-4ea2-afab-d19e32b21934"
      },
      "source": [
        "preds_df[preds_df[\"CountryName\"] == \"United States\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        CountryName RegionName       Date  PredictedDailyNewCases\n",
              "5425  United States        NaN 2020-08-01           168976.740204\n",
              "5426  United States        NaN 2020-08-02           217419.020776\n",
              "5427  United States        NaN 2020-08-03           205848.838913\n",
              "5428  United States        NaN 2020-08-04           219066.038361\n",
              "5429  United States        NaN 2020-08-05           165365.279051\n",
              "...             ...        ...        ...                     ...\n",
              "7063  United States    Wyoming 2020-08-27                0.000000\n",
              "7064  United States    Wyoming 2020-08-28              232.091698\n",
              "7065  United States    Wyoming 2020-08-29                0.000000\n",
              "7066  United States    Wyoming 2020-08-30                0.000000\n",
              "7067  United States    Wyoming 2020-08-31                0.000000\n",
              "\n",
              "[1643 rows x 4 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CountryName</th>\n",
              "      <th>RegionName</th>\n",
              "      <th>Date</th>\n",
              "      <th>PredictedDailyNewCases</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5425</th>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-08-01</td>\n",
              "      <td>168976.740204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5426</th>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-08-02</td>\n",
              "      <td>217419.020776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5427</th>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-08-03</td>\n",
              "      <td>205848.838913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5428</th>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-08-04</td>\n",
              "      <td>219066.038361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5429</th>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-08-05</td>\n",
              "      <td>165365.279051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7063</th>\n",
              "      <td>United States</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2020-08-27</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7064</th>\n",
              "      <td>United States</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2020-08-28</td>\n",
              "      <td>232.091698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7065</th>\n",
              "      <td>United States</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2020-08-29</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7066</th>\n",
              "      <td>United States</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2020-08-30</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7067</th>\n",
              "      <td>United States</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2020-08-31</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1643 rows  4 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrH4CpLILWtk"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Refactor [predict.py](https://github.com/leaf-ai/covid-xprize/blob/master/covid_xprize/examples/predictors/lstm/predict.py) to work with out model. Move to the team sandbox and get it to run there for the robojudge. Maybe not too helpful but you can see the validation section of this example for more background. [example link](https://github.com/leaf-ai/covid-xprize/blob/master/covid_xprize/examples/predictors/lstm/Example-LSTM-Predictor.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJDQqcuPMveT"
      },
      "source": [
        "# TODO get this API to run\n",
        "# !python predict.py -s 2020-08-01 -e 2020-08-04 -ip ../../../validation/data/2020-09-30_historical_ip.csv -o predictions/2020-08-01_2020-08-04.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVfEfc3lMHPX"
      },
      "source": [
        "##Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twlQBotsQrls"
      },
      "source": [
        "For Localhosts\n",
        "\n",
        "\n",
        "1) Start the remote server and run tensorboard on the server\n",
        "```bash\n",
        "tensorboard --logdir=./logs/ --host $SERVER_IP --port $SERVER_PORT\n",
        "```\n",
        "2) SSH tunnel the port to your laptop\n",
        "\n",
        "```bash\n",
        "ssh uname@hostname.edu -L 6006:$SERVER_IP:$SERVER_PORT\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_syDiInQibz"
      },
      "source": [
        "For Normal Jupyter notebooks (untested)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHUJYrgsMMph"
      },
      "source": [
        "# %tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jae_4nuHQfnw"
      },
      "source": [
        "For Colab (untested)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfLm9N0EMM5Q"
      },
      "source": [
        "# LOG_DIR = './log'\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )\n",
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5URLpMZ9QVGz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}